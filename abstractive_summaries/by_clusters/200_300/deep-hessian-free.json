{"blog_id": "deep-hessian-free", "summary": ["James Martens, 2010  This paper introduces a fairly complex optimization algorithm for deep nets that uses approximate 2nd-order gradient information  In Hessian-Free optimization, you can directly approximate a Hessian-vector product $Hv$ with the method of finite-differences; this only costs 1 more gradient evaluation  linear conjugate gradient algorithm allows one to solve for the optimal search direction in $O(N)$ iterations ($N$ is the number of parameters) with only matrix-vector products  Newton\u2019s method is scale invariant, e.g., for a new parameterization $\\hat{\\theta} = A \\theta$ for some invertible matrix $A$, the optimal search direction is now $\\hat{p} = A p$ where $p$ is the original optimal search direction.", "Gradient descent is not (need proof!)", "- so many bad things about GD, but it\u2019s so easy to implement..", "Considerations when applying this technique  Need to use an adaptive damping parameters $\\lambda$ beause the relative scale of $B = H(\\theta)$ is changing and $H(\\theta)$ must remain positive semidefinite.", "Recommended heuristic is given in Section 4.1  Gauss-Newton matrix $G$ can produce better search directions than $H$, see this blog post for a summary  Compute gradient on entire dataset, but use minibatches to compute Hessian-vector products.", "SGD requires 10\u2019s of thousands of iterations versus ~200 for HF"], "author_id": "pemami", "pdf_url": "http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf", "author_full_name": "Patrick Emami", "source_website": "https://pemami4911.github.io/index.html", "id": 4206973}