{"blog_id": "1409.7495", "summary": ["The goal of this method is to create a feature representation $f$ of an input $x$ that is domain invariant over some domain $d$.", "The feature vector $f$ is obtained from $x$ using an encoder network (e.g. $f = G_f(x)$).", "The reason this is an issue is that the input $x$ is correlated with $d$ and this can confuse the model to extract features that capture differences in domains instead of differences in classes.", "Here I will recast the problem differently from in the paper:  **Problem:** Given a conditional probability $p(x|d=0)$ that may be different from $p(x|d=1)$:  $$p(x|d=0) \\stackrel{?", "}{\\ne} p(x|d=1)$$  we would like it to be the case that these distributions are equal.", "$$p(G_f(x) |d=0) = p(G_f(x)|d=1)$$  aka:  $$p(f|d=0) = p(f|d=1)$$  Of course this is an issue if some class label $y$ is correlated with $d$ meaning that we may hurt the performance of a classifier that now may not be able to predict $y$ as well as before.", "[url]"], "author_id": "joecohen", "pdf_url": "http://arxiv.org/pdf/1409.7495v2", "author_full_name": "Joseph Cohen", "source_website": "https://www.shortscience.org/user?name=joecohen", "id": 93218559}