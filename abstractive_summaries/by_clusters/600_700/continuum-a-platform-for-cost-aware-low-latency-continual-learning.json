{"blog_id": "continuum-a-platform-for-cost-aware-low-latency-continual-learning", "summary": ["Continuum: a platform for cost-aware low-latency continual learning Tian et al., SoCC\u201918  Let\u2019s start with some broad approximations.", "Batching leads to higher throughput at the cost of higher latency.", "Processing items one at a time leads to lower latency and often reduced throughput.", "We can recover throughput to a degree by throwing horizontally scalable resources at the problem, but it\u2019s hard to recover latency.", "In many business scenarios latency matters, so we\u2019ve been seeing a movement overtime from batching through micro-batching to online streaming.", "Continuum looks at the same issues from the perspective of machine learning models.", "Offline (batch) trained models can suffer from concept drift (loss of accuracy over time) as a result of not incorporating the latest data.", "I.e., there\u2019s a business cost incurred for higher latency of update incorporation.", "Online models support incremental updates.", "Continuum determines the optimum time to retrain models in the presence of incoming data, based on user policy (best effort, cost-aware, or user-defined).", "There\u2019s some great data here about the need for and benefit of continual learning, and a surprising twist in the tale where it turns out that even if you can afford it, updating the model on every incoming data point is not the best strategy even when optimising for lowest latency of update incorporation.", "When good models go stale  There are a number of purpose-designed online learning algorithms (e.g. Latent Dirichlet Allocation for topic models, matrix factorization for recommender systems, and Bayesian inference for stream analytics).", "However, many mainstream ML frameworks including TensorFlow, MLib, XGBoost, scikit-learn and MALLET do not explicitly support continual model updating.", "It\u2019s up to the user to code custom training loops to manually trigger retraining.", "Often such models are updated on much slower timescales (e.g. daily, or maybe hourly) than the generation of the data they operate over.", "This causes a loss in model accuracy when concept drift occurs.", "Consider a Personalized PageRank (PPR) algorithm being fed data from Twitter.", "The following chart shows how L1 error and MAP metrics degrade over time as compared to a model trained on the very most recent data.", "The base model decays by about 10% in one hour.", "Using offline (batch) training to retrain the model from scratch every 10 minutes also takes orders of magnitude more compute than online learning, making short retraining windows using the full training data set impractical.", "When a topic modelling model (also trained using tweets) is updated once every ten minutes, we can see that it\u2019s perplexity (lower is better) decreases with every retraining.", "The performance of an ad recommendation system classifier similarly improves over time with model updating every five minutes.", "Data recency clearly matters in a number of applications.", "But writing your own continual learning training loops can be as much if not more work than implementing the key logic of online learning in the first place.", "An overview of Continuum  Continuum is designed to support continual learning a cross a broad set of ML frameworks.", "Based on an update policy, Continuum decides when to update the model.", "At runtime it looks like this:  Clients send data updates (or pointers to updated data) to Continuum  Continuum stores the updated data  Continuum evaluates the update policy and triggers retraining if needed  The backend fetches the updated data from data storage and trains the model  The backend notifies Continuum that an updated model is available.", "The updated model can then be shipped to a model serving system, e.g. Clipper .", "Continuum is about 4,000 lines of C++ and Python, and is available in open-source here:  [url]"], "author_id": "ACOLYER", "pdf_url": "https://www.cse.ust.hk/~weiwa/papers/huangshi-socc18.pdf", "author_full_name": "Adrian Colyer", "source_website": "https://blog.acolyer.org/about/", "id": 78499306}