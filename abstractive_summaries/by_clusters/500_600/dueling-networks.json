{"blog_id": "dueling-networks", "summary": ["For many states, it is unnecessary to estimate the action value for each action.", "This is a problem with methods that attempt to favor exploration over exploitation too much, because often times there will be a large number of actions that have little to no value for a given state.", "The Q-Network in this novel architecture is decomposed into two separate streams; a value stream and an advantage stream.", "Feature learning is carried out by a number of convolutional and pooling layers.", "The activations of the last of these layers are sent to both separate streams.", "Each stream contains a number of fully-connected layers.", "The final layer combines the output of the two streams, and the output of the network is a set of Q values, one for each action.", "The aggregator for the two outputs of the advantage and value streams is:  $\\beta$ refers to the parameters specific to the value network, and the alpha refers to the parameters specific to the advantage network.", "The advantage of the dueling network over standard Q-Networks is especially prominent when the number of actions is large.", "For standard Q-Networks, when the variation between actions is small, the Q-Network effectively has to learn the same value for all actions while each update only modifies the Q value of one action.", "Evidence  The dueling network architecture outperformed the Double-DQN results in 50 out of 57 learned Atari games.", "Strengths  The paper does a good job of making its main contribution (a novel neural network architecture) clear at the beginning.", "The experimental algorithm for Dueling Networks employs other state-of-the-art advances in DRL (such as Double-DQN) which helps show the correlation between research being carried on in this field.", "Interesting related works  Increasing the action gap (Bellemare et al., 2016)  Notes  The value function V measures the importance of being in a particular state s. The Q-function measures the importance about the value of choosing each possible action when in this state.", "The advantage function, $A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$, subtracts the value of the state from the Q-function to obtain a relative measure of the importance of each action  A deep Q-network is a non-linear function approximator for the Q function having the form $Q(s,a;\\theta)$ with parameters theta  We optimize the following sequence of loss functions:  with the target of the loss function, $y_i^{DQN}$, given by the reward signal plus the discounted maximal Q value provided by the target Q-Network  A target Q-Network that uses parameter freezing for a certain number of iterations is used to stabilize the algorithm  The specific gradient is  Experience replay is used; use prioritized experience replay instead!", "(Don\u2019t just sample uniformly from memory)  To avoid over-optimistic value estimates (van Hasselt, 2010), use Double Q-Learning.", "Originally, the max operator uses the same values to both select and evaluate an action.", "Instead, use the following target:"], "author_id": "pemami", "pdf_url": "http://arxiv.org/pdf/1511.06581", "author_full_name": "Patrick Emami", "source_website": "https://pemami4911.github.io/index.html", "id": 33427271}