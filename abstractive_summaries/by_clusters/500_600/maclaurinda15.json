{"blog_id": "maclaurinda15", "summary": ["This is another \"learning the learning rate\" paper, which predates (and might have inspired) the \"Speed learning on the fly\" paper I recently wrote notes about (see  [ref] ).", "In this paper, they consider the off-line training scenario, and propose to do gradient descent on the learning rate by unrolling the *complete* training procedure and treating it all as a function to optimize, with respect to the learning rate.", "This way, they can optimize directly the validation set loss.", "The paper in fact goes much further and can tune many other hyper-parameters of the gradient descent procedure: momentum, weight initialization distribution parameters, regularization and input preprocessing.", "#### My two cents  This is one of my favorite papers of this year.", "While the method of unrolling several steps of gradient descent (100 iterations in the paper) makes it somewhat impractical for large networks (which is probably why they considered 3-layer networks with only 50 hidden units per layer), it provides an incredibly interesting window on what are good hyper-parameter choices for neural networks.", "Note that, to substantially reduce the memory requirements of the method, the authors had to be quite creative and smart about how to encode changes in the network's weight changes.", "There are tons of interesting experiments, which I encourage the reader to go check out (see section 3).", "One experiment on training the learning rates, separately for each iteration (i.e. learning a learning rate schedule), for each layer and for either weights or biases (800 hyper-parameters total) shows that a good schedule is one where the top layer first learns quickly (large learning), then the bottom layer starts training faster, and finally the learning rates of all layers is decayed towards zero.", "Note that some of the experiments presented actually optimized the training error, instead of the validation set error.", "Another looked at finding optimal scales for the weight initialization.", "Interestingly, the values found weren't that far from an often prescribed scale of $1 / \\sqrt{N}$, where $N$ is the number of units in the previous layer.", "The experiment on \"training the training set\", i.e. generating the 10 examples (one per class) that would minimize the validation set loss of a network trained on these examples is a pretty cool idea (it essentially learns prototypical images of the digits from 0 to 9 on MNIST).", "Another experiment tried to optimize a multitask regularization matrix, in order to encourage forms of soft-weight-tying across tasks.", "Note that approaches like the one in this paper make tools for automatic differentiation incredibly valuable.", "Python autograd, the author's automatic differentiation Python library  [url]"], "author_id": "hlarochelle", "pdf_url": "http://proceedings.mlr.press/v37/maclaurin15.pdf", "author_full_name": "Hugo Larochelle", "source_website": "https://www.shortscience.org/user?name=hlarochelle", "id": 72788236}