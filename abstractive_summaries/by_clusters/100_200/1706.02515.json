{"blog_id": "1706.02515", "summary": ["\"Using the \"SELU\" activation function, you get better results than any other activation function, and you don't have to do batch normalization.", "The \"SELU\" activation function is:  if x<0, 1.051\\*(1.673\\*e^x-1.673) if x>0, 1.051\\*x\" Source: narfon2, reddit   ``` import numpy as np  def selu(x):     alpha = 1.6732632423543772848170429916717     scale = 1.0507009873554804934193349852946     return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha) ``` Source: CaseOfTuesday, reddit  Discussion here:  [url]"], "author_id": "joecohen", "pdf_url": "http://arxiv.org/pdf/1706.02515v1", "author_full_name": "Joseph Cohen", "source_website": "https://www.shortscience.org/user?name=joecohen", "id": 8240954}