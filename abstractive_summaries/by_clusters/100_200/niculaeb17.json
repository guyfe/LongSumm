{"blog_id": "niculaeb17", "summary": ["The idea in this paper is to develop a version of attention that will incorporate similarity in neighboring bins.", "This aligned with the work  [ref]  which presented a different approach to deal with consistency between classes of predictions.", "In this work the closed form softmax function is replaced by a small optimization problem with this regularizer:  $$ +\\lambda \\sum_{i=1}^{d-1} |y_{i+1}-y_i|$$  Because of this, many of the neighboring probabilities are exactly the same resulting in attention that can be seen as blocks.", "[url]"], "author_id": "joecohen", "pdf_url": "http://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf", "author_full_name": "Joseph Cohen", "source_website": "https://www.shortscience.org/user?name=joecohen", "id": 74374635}