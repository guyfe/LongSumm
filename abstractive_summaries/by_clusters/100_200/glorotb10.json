{"blog_id": "glorotb10", "summary": ["The weights at each layer $W$ are initialized based on the number of connections they have.", "Each $w \\in W$  is drawn from a Gaussian distribution with mean $\\mu = 0$ with the variance as follows.", "$$\\text{Var}(W) = \\frac{2}{n_\\text{in}+ n_\\text{out}}$$  Where $n_\\text{in}$ is the number of neurons in the previous layer from the feedforward direction and $n_\\text{out}$ is the number of neurons from the previous layer from the backprop direction.", "Reference: [Andy Jones's Blog]( [url]"], "author_id": "joecohen", "pdf_url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf", "author_full_name": "Joseph Cohen", "source_website": "https://www.shortscience.org/user?name=joecohen", "id": 70140653}