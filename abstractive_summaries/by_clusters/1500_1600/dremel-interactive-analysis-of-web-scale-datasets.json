{"blog_id": "dremel-interactive-analysis-of-web-scale-datasets", "summary": ["Dremel: interactive analysis of web-scale datasets \u2013 Melnik et al. (Google), 2010.", "Dremel is Google\u2019s interactive ad-hoc query system that can run aggregate queries over trillions of rows in seconds.", "It scales to thousands of CPUs, and petabytes of data.", "It was also the inspiration for Apache Drill.", "Dremel borrows the idea of serving trees from web search (pushing a query down a tree hierarchy, rewriting it at each level and aggregating the results on the way back up).", "It uses a SQL-like language for query, and it uses a column-striped storage representation.", "It also supports a nested data model \u2013 Google\u2019s Protocol Buffers  Column stores have been adopted for analyzing relational data [1] but to the best of our knowledge have not been extended to nested data models.", "The columnar storage format that we present is supported by many data processing tools at Google, including MR, Sawzall, and FlumeJava.", "The way in which nested data structures are split into columns, and then re-assembled on demand in response to queries is central to Dremel, so let\u2019s explore how that works.", "Here\u2019s a sample Protocol Buffer schema for a Document entity:  message Document {     required int64 DocId;     optional group Links {         repeated int64 Backward;         repeated int64 Forward;     }     repeated group Name {         repeated group Language {             required string Code;             optional string Country;         }         optional String Url;     } }  Notice a few things about this: there are repeated and optional components, and there is nesting.", "The first part of splitting this into columns is pretty straight-forward: make a column for each field, using the nested path names.", "So, for the schema above we have columns DocId, Links.Backward, Links.Forward, Name.Language.Code, Name.Language.Country, and Name.Url.", "Focusing in on the Name.Language.Code column, we\u2019re going to take the Code entries from multiple \u2018rows\u2019 (Documents) and put them all into this column.", "The first problem we have to solve comes from the fact that \u2018Code\u2019 can be repeated several times within the same Document.", "In the \u2018DocId\u2019 column, each entry  represents a new Document, but in the Name.Language.Code column we need a way to know whether a given entry is a repeated entry from the current Document, or the start of a new Document.", "And if it is repeated,  where does it belong in the nesting structure?", "Furthermore, given the fact that some fields are optional (may be missing), we\u2019re going to need a way to take that into account too.", "Dremel solves these problems by keeping three  pieces of data for every column entry: the value itself, a repetition level, and a definition level.", "How it works is pretty subtle to wrap your head around, but I\u2019ll try to explain it as clearly as possible.", "Take a good look at the sketch below from my notebook.", "It shows a Document record that we want to split into columns, and to the right, the column entries that result within the Name.Language.Code column \u2013 where r represents the repetition level, and d the definition level.", "The first problem we mentioned was how to tell whether an entry is the start of a new Document, or another entry for the same column within the current Document.", "That\u2019s an easy one to solve: the repetition level is set to 0 for the first occurence of a field within a record.", "Hence \u2018en-us,\u2019 which is the first Code within the document, is encoded with repetition level 0.", "You might intuitively think that we\u2019d simply increment the repitition level for each occurence (and hence the \u2018en\u2019 entry would have repetition level 1, and \u2018en-gb\u2019 repetitionlevel 2).", "But that\u2019s not actually how it works.", "Notice that if we did that, we couldn\u2019t distinguish between a Code that is in a repeated Language element of a given Name (\u2018en\u2019 in our example), and a Code that is in a new Name element (the \u2018en-gb\u2019 case).", "So for all repeated column entries in a record after the first one, the repetition level value that is stored is instead the \u2018level\u2019 at which we\u2019re repeating.", "For the nesting Name.Language.Code, Name is level 1, Language is level 2, and Code is level 3.", "Still with me?", "Good, so, when we come to encode the \u2018en\u2019 value in the column, we give it repetition level 2, because it is inside a replicated Language element.", "And when we come to encode the \u2018en-gb\u2019 value, we give it repetition level 1, because Name is the first level at which we\u2019re repeating at this point in the record.", "And that NULL value you see in the column?", "That\u2019s there to record the fact that the second Name element doesn\u2019t include a Language.Code value at all.", "It\u2019s at repetition level 1 because the \u2018Name\u2019 element is the level we\u2019re repeating at.", "Now let\u2019s talk about the definition level value, d. Intuitively you might think this is just the nesting level in the schema (so 1 for DocId,  2 for Links.Forward, 3 for Name.Language.Code etc.)", "\u2013 but again that\u2019s not quite what it represents (and in fact, if we had access to the schema, it would be redundant to store that information with every column entry of course).", "Instead, the definition level indicates how many of the parent fields are actually defined.", "This is easier to understand by example.", "For the \u2018en-us\u2019 Code entry, it\u2019s within a Language field, within a Name field \u2013 so this gets definition level 2.", "The same is true for the \u2018en\u2019 and \u2018en-gb\u2019 entries.", "For the NULL entry though, the enclosing \u2018Name\u2019 is present, but there is no \u2018Language\u2019 component at all.", "Therefore this gets definition level 1.", "It turns out that by encoding these repitition and definition levels alongside the column value, it is possible to split records into columns, and subsequently re-assemble them efficiently.", "The algorithms for doing this are given in an appendix to the paper.", "Record assembly is pretty neat \u2013 for the subset of the fields the query is interested in, a Finite State Machine is generated with state transitions triggered by changes in repetition level.", "Let\u2019s cut to the chase now and look at what Google learned building and operating Dremel:  Scan-based queries can be executed at interactive speeds on disk-resident datasets of up to a trillion records.", "Near-linear scalability in the number of columns and servers is achievable for systems containing thousands of nodes.", "MR can benefit from columnar storage just like a DBMS.", "Record assembly and parsing are expensive.", "Software layers (beyond the query processing layer) need to be optimized to directly consume column-oriented data.", "MR and query processing can be used in a complementary fashion; one layer\u2019s output can feed another\u2019s input.", "In a multi-user environment, a larger system can benefit from economies of scale while offering a qualitatively better user experience.", "(Splitting the work into more parallel pieces reduced overall response time, without causing more underlying resource, e.g. CPU,  consumption)  If trading speed against accuracy is acceptable, a query can be terminated much earlier and yet see most of the data.", "The bulk of a web-scale dataset can be scanned fast.", "Getting to the last few percent within tight time bounds is hard.", "The last two points relate to the problems we looked at in The Tail at Scale \u2013 the outliers are always slower, and at enough scale you\u2019re going to get tripped up by that.", "Dremel allows you to specify the percentage of data processed at which you\u2019re happy to stop a query and return results.", "It sounds odd to say you want the results of a query without looking at all of the data \u2013 but consider for example a top-k query.", "Looking at 98% of the data makes it highly likely you\u2019ll get the right answer,  and cutting off the slow tail can give a response in under a minute as opposed to several minutes just waiting for that last 2%."], "author_id": "ACOLYER", "pdf_url": "http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf", "author_full_name": "Adrian Colyer", "source_website": "https://blog.acolyer.org/about/", "id": 21145042}