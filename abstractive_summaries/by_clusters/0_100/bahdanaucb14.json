{"blog_id": "bahdanaucb14", "summary": ["One core aspect of this attention approach is that it provides the ability to debug the learned representation by visualizing the softmax output (later called $\\alpha_{ij}$) over the input words for each output word as shown below.", "[url]"], "author_id": "joecohen", "pdf_url": "http://arxiv.org/pdf/1409.0473", "author_full_name": "Joseph Cohen", "source_website": "https://www.shortscience.org/user?name=joecohen", "id": 24431610}