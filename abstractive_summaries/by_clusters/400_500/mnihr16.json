{"blog_id": "mnihr16", "summary": ["This paper explores the use of so-called Monte Carlo objectives for training directed generative models with latent variables.", "Monte Carlo objectives take the form of the logarithm of a Monte Carlo estimate (i.e. an average over samples) of the marginal probability $P(x)$.", "One important motivation for using Monte Carlo objectives is that they can be shown (see the Importance Weighted Variational Autoencoder paper  [ref]  and my notes on it) to correspond to bounds on the true likelihood of the model, and one can tighten the bound simply by drawing more samples in the Monte Carlo objective.", "Currently, the most successful application of Monte Carlo objectives is based on an importance sampling estimate, which involves training a proposal distribution $Q(h|x)$ in addition to the model $P(x,h)$.", "This paper considers the problem of training with gradient descent on such objectives, in the context of a model to which the reparametrization trick cannot be used (e.g. for discrete latent variables).", "They analyze the sources of variance in the estimation of the gradients (see Equation 5) and propose a very simple approach to reducing the variance of a sampling-based estimator of these gradients.", "First, they argue that gradients with respect to the $P(x,h)$ parameters are less susceptible to problems due to high variance gradients.", "Second, and most importantly, they derive a multi-sample estimate of the gradient that is meant to reduce the variance of gradients on the proposal distribution parameters $Q(h|x)$.", "The end result is the gradient estimate of Equations 10-11.", "It is based on the observation that the first term of the gradient of Equation 5 doesn't distinguish between the contribution of each sampled latent hi.", "The key contribution is this: they notice that one can incorporate a variance reducing baseline for each sample hi, corresponding to the Monte Carlo estimate of the log-likelihood when removing hi from the estimate (see Equation 10).", "The authors show that this is a proper baseline, in that using it doesn't introduce a bias in the estimation for the gradients.", "Experiments show that this approach yields better performance than training based on Reweighted Wake Sleep  [ref]  or the use of NVIL baselines  [ref] , when training sigmoid belief networks as generative models or as structured output prediction (image completion) models on binarized MNIST."], "author_id": "hlarochelle", "pdf_url": "http://arxiv.org/pdf/1602.06725", "author_full_name": "Hugo Larochelle", "source_website": "https://www.shortscience.org/user?name=hlarochelle", "id": 10218080}